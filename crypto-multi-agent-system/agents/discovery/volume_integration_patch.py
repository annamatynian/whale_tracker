"""
Volume Integration Patch - –¥–æ–±–∞–≤–ª—è–µ—Ç volume metrics –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π discovery pipeline
–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ –∏–Ω–≤–∞–∑–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Part4

Author: Phase 1 Volume Acceleration integration
Version: 1.0
"""

import asyncio
import logging
import requests
from typing import Dict, Any, Optional
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–≥–æ –Ω–æ–≤–æ–≥–æ –º–æ–¥—É–ª—è
from agents.discovery.volume_metrics_extension import (
    build_token_day_data_query,
    calculate_volume_metrics_from_daily_data,
    prepare_day_data_variables,
    apply_volume_filters
)


class VolumeMetricsFetcher:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ —Ä–∞—Å—á–µ—Ç–∞ volume metrics –¥–ª—è –ø–∞—Ä.
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π TheGraph.
    """
    
    def __init__(self, graph_api_key: str, graph_gateway_base: str = "https://gateway.thegraph.com/api"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fetcher.
        
        Args:
            graph_api_key: The Graph API key
            graph_gateway_base: Base URL –¥–ª—è gateway
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.graph_api_key = graph_api_key
        self.graph_gateway_base = graph_gateway_base
        self.query_template = build_token_day_data_query()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "pairs_with_acceleration": 0,
            "pairs_filtered_out": 0
        }
    
    def _build_subgraph_url(self, subgraph_id: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è subgraph."""
        return f"{self.graph_gateway_base}/{self.graph_api_key}/subgraphs/id/{subgraph_id}"
    
    async def fetch_token_day_data(
        self, 
        token_address: str, 
        subgraph_id: str,
        days_back: int = 30
    ) -> Optional[Dict[str, Any]]:
        """
        –ó–∞–ø—Ä–æ—Å–∏—Ç—å tokenDayData –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.
        
        Args:
            token_address: –∞–¥—Ä–µ—Å —Ç–æ–∫–µ–Ω–∞
            subgraph_id: ID subgraph –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            days_back: —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å
            
        Returns:
            Dict —Å volume metrics –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            self.stats["total_requests"] += 1
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å URL –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            url = self._build_subgraph_url(subgraph_id)
            variables = prepare_day_data_variables(token_address, days_back)
            
            # –°–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: requests.post(
                    url,
                    json={"query": self.query_template, "variables": variables},
                    headers={"Content-Type": "application/json"},
                    timeout=15
                )
            )
            
            if response.status_code == 200:
                data = response.json()
                
                if "errors" not in data:
                    token_day_data = data.get("data", {}).get("tokenDayDatas", [])
                    
                    if token_day_data:
                        # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
                        metrics = calculate_volume_metrics_from_daily_data(token_day_data)
                        
                        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
                        passed, reason = apply_volume_filters(metrics)
                        
                        # –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        self.stats["successful_requests"] += 1
                        if metrics['is_accelerating']:
                            self.stats["pairs_with_acceleration"] += 1
                        if not passed:
                            self.stats["pairs_filtered_out"] += 1
                        
                        return {
                            "success": True,
                            "metrics": metrics,
                            "passed_filters": passed,
                            "filter_reason": reason,
                            "raw_data_points": len(token_day_data)
                        }
                    else:
                        self.logger.debug(f"No tokenDayData found for {token_address[:10]}...")
                        self.stats["failed_requests"] += 1
                        return None
                else:
                    error_msg = data['errors'][0]['message']
                    self.logger.warning(f"GraphQL error for {token_address[:10]}...: {error_msg}")
                    self.stats["failed_requests"] += 1
                    return None
            else:
                self.logger.warning(f"HTTP {response.status_code} for {token_address[:10]}...")
                self.stats["failed_requests"] += 1
                return None
                
        except Exception as e:
            self.logger.error(f"Error fetching volume data for {token_address[:10]}...: {e}")
            self.stats["failed_requests"] += 1
            return None
    
    async def enrich_discovery_report_with_volume_metrics(
        self,
        discovery_report: Any,  # TokenDiscoveryReport
        subgraph_id: str
    ) -> Any:
        """
        –û–±–æ–≥–∞—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π discovery report volume –º–µ—Ç—Ä–∏–∫–∞–º–∏.
        
        Args:
            discovery_report: TokenDiscoveryReport –æ–±—ä–µ–∫—Ç
            subgraph_id: ID subgraph –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π TokenDiscoveryReport —Å volume_metrics –∞—Ç—Ä–∏–±—É—Ç–æ–º
        """
        # –ó–∞–ø—Ä–æ—Å–∏—Ç—å volume –¥–∞–Ω–Ω—ã–µ –¥–ª—è BASE TOKEN (–Ω–µ –ø–∞—Ä—ã!)
        volume_data = await self.fetch_token_day_data(
            discovery_report.base_token_address,  # –í–ê–ñ–ù–û: —Ç–æ–∫–µ–Ω, –Ω–µ –ø–∞—Ä–∞!
            subgraph_id
        )
        
        if volume_data and volume_data["success"]:
            # –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ report (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)
            discovery_report.volume_metrics = volume_data["metrics"]
            discovery_report.volume_filters_passed = volume_data["passed_filters"]
            discovery_report.volume_filter_reason = volume_data["filter_reason"]
            
            metrics = volume_data["metrics"]
            bonus_points = 0
            bonus_reasons = []
            
            # –ë–æ–Ω—É—Å 1: –£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ (+10-15 –±–∞–ª–ª–æ–≤)
            if metrics["is_accelerating"]:
                acceleration_bonus = min(15, int(metrics["acceleration_factor"] * 10))
                bonus_points += acceleration_bonus
                bonus_reasons.append(f"üî• Volume acceleration {metrics['acceleration_factor']:.2f}x (+{acceleration_bonus})")
            
            # –ë–æ–Ω—É—Å 2: Volume Ratio Health Check (+5 –±–∞–ª–ª–æ–≤ –∑–∞ –∑–¥–æ—Ä–æ–≤—ã–π ratio)
            if metrics.get("volume_ratio_healthy", False):
                bonus_points += 5
                bonus_reasons.append(f"‚úÖ Healthy volume ratio {metrics['volume_ratio']:.2f} (+5)")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–≥—Ä–µ–≤–µ (–Ω–µ —à—Ç—Ä–∞—Ñ—É–µ–º, –Ω–æ –æ—Ç–º–µ—á–∞–µ–º)
            if metrics.get("volume_ratio_overheated", False):
                bonus_reasons.append(f"‚ö†Ô∏è Unusually high volume ratio {metrics['volume_ratio']:.2f}")
            
            # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –±–æ–Ω—É—Å—ã
            if bonus_points > 0:
                discovery_report.discovery_score += bonus_points
                discovery_report.discovery_reason += "; " + "; ".join(bonus_reasons)
        else:
            # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö - –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            discovery_report.volume_metrics = None
            discovery_report.volume_filters_passed = False
            discovery_report.volume_filter_reason = "No historical data available"
        
        return discovery_report
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã."""
        stats = self.stats.copy()
        
        if stats["total_requests"] > 0:
            stats["success_rate"] = (stats["successful_requests"] / stats["total_requests"]) * 100
            
            if stats["successful_requests"] > 0:
                stats["acceleration_rate"] = (stats["pairs_with_acceleration"] / stats["successful_requests"]) * 100
                stats["filter_pass_rate"] = ((stats["successful_requests"] - stats["pairs_filtered_out"]) / stats["successful_requests"]) * 100
        
        return stats


# === –ü–ê–¢–ß –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –° PART4 ===

async def patch_part4_with_volume_metrics(
    discovery_reports: list,
    subgraph_id: str,
    graph_api_key: str,
    max_concurrent: int = 3
) -> tuple[list, dict]:
    """
    –ü–∞—Ç—á –¥–ª—è Part4: –¥–æ–±–∞–≤–ª—è–µ—Ç volume metrics –∫–æ –≤—Å–µ–º discovery reports.
    
    Args:
        discovery_reports: —Å–ø–∏—Å–æ–∫ TokenDiscoveryReport –æ–±—ä–µ–∫—Ç–æ–≤
        subgraph_id: ID subgraph
        graph_api_key: API key
        max_concurrent: –º–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
    Returns:
        (enriched_reports, stats)
    """
    logger = logging.getLogger("VolumeMetricsPatch")
    
    if not discovery_reports:
        logger.warning("No discovery reports to enrich")
        return [], {}
    
    logger.info(f"Enriching {len(discovery_reports)} reports with volume metrics...")
    
    # –°–æ–∑–¥–∞—Ç—å fetcher
    fetcher = VolumeMetricsFetcher(graph_api_key)
    
    # –û–±–æ–≥–∞—Ç–∏—Ç—å –∫–∞–∂–¥—ã–π report (—Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞)
    enriched_reports = []
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (–¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
    for i, report in enumerate(discovery_reports):
        try:
            if (i + 1) % 10 == 0:
                logger.info(f"  Progress: {i + 1}/{len(discovery_reports)}")
            
            enriched_report = await fetcher.enrich_discovery_report_with_volume_metrics(
                report, 
                subgraph_id
            )
            enriched_reports.append(enriched_report)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(0.3)
            
        except Exception as e:
            logger.error(f"Failed to enrich report {i}: {e}")
            enriched_reports.append(report)  # –î–æ–±–∞–≤–ª—è–µ–º –±–µ–∑ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
    
    # –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = fetcher.get_stats()
    
    logger.info(f"Volume enrichment complete: {stats['successful_requests']}/{stats['total_requests']} successful")
    logger.info(f"  Pairs with acceleration: {stats.get('pairs_with_acceleration', 0)}")
    logger.info(f"  Pairs filtered out: {stats.get('pairs_filtered_out', 0)}")
    
    return enriched_reports, stats


# === –¢–ï–°–¢–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø ===

async def test_volume_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º subgraph (–µ—Å–ª–∏ –µ—Å—Ç—å API key)."""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    api_key = os.getenv("GRAPH_API_KEY")
    uniswap_v2_id = os.getenv("UNISWAP_V2_ID")
    
    if not api_key or not uniswap_v2_id:
        print("‚ö†Ô∏è GRAPH_API_KEY or UNISWAP_V2_ID not found in .env")
        print("   Skipping real API test")
        return
    
    print("=" * 60)
    print("TEST: Volume Integration with Real API")
    print("=" * 60)
    
    fetcher = VolumeMetricsFetcher(api_key)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∞–¥—Ä–µ—Å —Ç–æ–∫–µ–Ω–∞ (USDC –Ω–∞ Ethereum)
    # –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ - –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –º—ã –ø–æ–ª—É—á–∞–µ–º –∞–¥—Ä–µ—Å–∞ –∏–∑ discovery
    test_token = "0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48"  # USDC
    
    print(f"\nFetching volume data for token: {test_token[:10]}...")
    
    volume_data = await fetcher.fetch_token_day_data(test_token, uniswap_v2_id)
    
    if volume_data:
        print("\n‚úì Successfully fetched volume data:")
        print(f"   Raw data points: {volume_data['raw_data_points']}")
        
        metrics = volume_data['metrics']
        print(f"\n   Metrics:")
        print(f"      avg_7d: ${metrics['avg_volume_last_7_days']:,.0f}")
        print(f"      avg_30d: ${metrics['avg_volume_last_30_days']:,.0f}")
        print(f"      acceleration: {metrics['acceleration_factor']:.2f}x")
        print(f"      is_accelerating: {metrics['is_accelerating']}")
        print(f"      volume_ratio: {metrics['volume_ratio']:.3f}")
        print(f"      ratio_healthy: {metrics['volume_ratio_healthy']}")
        
        print(f"\n   Filter result: {'‚úì PASS' if volume_data['passed_filters'] else '‚úó FAIL'}")
        print(f"   Reason: {volume_data['filter_reason']}")
    else:
        print("\n‚úó Failed to fetch volume data")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = fetcher.get_stats()
    print(f"\n   Fetcher stats: {stats}")
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç
    asyncio.run(test_volume_integration())
